{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from data_transform_pipeline import DataTransformPipeline\n",
    "from data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorboard_logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds, idx_word_map, embedding_vocab = DataTransformPipeline.load(\"f_sents_prod\").data\n",
    "embedding_vocab = torch.stack(list(embedding_vocab.values()))\n",
    "\n",
    "y, mapping = DataTransformPipeline.load(\"genre2_label _prod\").data\n",
    "y = y.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24603, 24603)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y), len(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(embeds, y, test_size=0.15, random_state = 42)\n",
    "\n",
    "train_data = Data(X_train, y_train)\n",
    "train_loader = train_data.get_loader(batch_size = 15)\n",
    "\n",
    "val_data = Data(X_test, y_test)\n",
    "val_loader = val_data.get_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20912, 20912)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding, embedding_size, hidden_size, num_layers, num_classes):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding)\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        x = x.long()\n",
    "\n",
    "        # get embedding of characters\n",
    "        embed = self.embedding(x)\n",
    "        # pack padded sequence\n",
    "#         embed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.numpy(), batch_first=True)\n",
    "        # fprop though RNN\n",
    "        rnn_out, self.hidden = self.rnn(embed, self.hidden)\n",
    "        # undo packing\n",
    "#         rnn_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        # sum hidden activations of RNN across time\n",
    "        rnn_out = torch.sum(rnn_out, dim=1)\n",
    "\n",
    "        logits = self.linear(rnn_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for lengths, labels, data in loader:\n",
    "        data_batch, lengths_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, lengths_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, logger = None):\n",
    "    learning_rate = 3e-5\n",
    "    num_epochs = 8 # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    step_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}\".format(epoch+1))\n",
    "        for i, (lengths, labels, data) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(data, lengths)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            logger.log_value(\"loss\", loss.item(), step_counter)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i > 0 and i % 50 == 0:\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                logger.log_value(\"validation accuracy\", val_acc, step_counter)\n",
    "#                 print('Epoch: [{}/{}] \\t Step: [{}/{}] \\tValidation Acc: [{:.4f}]'.format(\n",
    "#                            epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            step_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(train_loader, val_loader, hidden_size, num_layers, logger = None):\n",
    "    model = RNN(embedding_vocab, embedding_vocab.shape[1], hidden_size, num_layers, np.unique(y).shape[0])\n",
    "    train(train_loader, val_loader, model, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src='http://127.0.0.1:6006/' width=1100 height=800></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "url = \"http://127.0.0.1:6006/\"\n",
    "iframe = \"<iframe src='{}' width=1100 height=800></iframe>\".format(url)\n",
    "IPython.display.HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Training Parameters\n",
      "-------------------\n",
      "\n",
      "hidden size: 350\n",
      "num layers: 3\n",
      "-------------------\n",
      "Epoch 1\n"
     ]
    }
   ],
   "source": [
    "hidden_size = [350, 350, 350, 350, 400, 400, 400, 400, 500, 500, 500, 500]\n",
    "num_layers =  [  3,   4,   5,   6,   3,   4,   5,   6,   3,   4,   5,   6]\n",
    "\n",
    "for i, params in enumerate(zip(hidden_size, num_layers)):\n",
    "    t = [\"{}: {}\".format(name, val) for name, val in [(\"hidden size\", params[0]), (\"num layers\", params[1])]]\n",
    "    print(\"-------------------\\nTraining Parameters\\n-------------------\\n\\n{}\\n-------------------\".format(\"\\n\".join(t)))\n",
    "    train_rnn(train_loader, val_loader, *params, Logger(\"runs/{}\".format(\"_\".join(t))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
