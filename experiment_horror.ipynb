{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from data_transform_pipeline import DataTransformPipeline\n",
    "from data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tensorboard_logger import Logger\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (14,9)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('horror.pkl', 'rb') as pickle_file:\n",
    "    horror_df = pkl.load(pickle_file)\n",
    "with open('not_horror.pkl', 'rb') as pickle_file:\n",
    "    other_df = pkl.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "horror_stories = list(horror_df['story'])\n",
    "other_stories = list(other_df['story'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(horror_stories)):\n",
    "    horror_stories[i] = horror_stories[i][:MAX_STORY_LENGTH]\n",
    "for i in range(len(other_stories)):  \n",
    "    other_stories[i] = other_stories[i][:MAX_STORY_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = random.randint(1, 100)\n",
    "random.Random(seed).shuffle(other_stories)\n",
    "other_stories = other_stories[:len(horror_stories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = horror_stories + other_stories\n",
    "labels = [0]*len(stories)\n",
    "for i in range(len(horror_stories)):\n",
    "    labels[i] = 1\n",
    "    \n",
    "seed = random.randint(1, 100)\n",
    "random.Random(seed).shuffle(stories)\n",
    "random.Random(seed).shuffle(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_categories = ['Horror']\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "MAX_STORY_LENGTH = 200\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "romance_content = content.loc[content['genre1'].isin(genre_categories)]\n",
    "romance_stories = list(romance_content['story'])\n",
    "other_content = content.loc[~content['genre1'].isin(genre_categories)]\n",
    "full_other_stories = list(other_content['story'])\n",
    "seed = random.randint(1, 100)\n",
    "random.Random(seed).shuffle(full_other_stories)\n",
    "other_stories = full_other_stories[0:len(romance_stories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = romance_stories + other_stories\n",
    "labels = [0]*len(stories)\n",
    "for i in range(len(romance_stories)):\n",
    "    labels[i] = 1\n",
    "\n",
    "seed = random.randint(1, 100)\n",
    "random.Random(seed).shuffle(stories)\n",
    "random.Random(seed).shuffle(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\"Okay, first we…\" Matt took a second to read the instructions. \"Set the two bed rails down on the part of the floor where you want the bed to go.\"\"Right.\" Scott said. He grunted as he lifted the rails to move them to the corner of the room. \"What now?\" he asked.\"Err… screw the headboard and rails together.\" He said.Just a few minutes ago Matt had been writing in his diary about Scott when a huge bang came from the twins' room. Matt had run inside to see tools scattered around and Scott clutching his foot in pain \"DAMN TOOL BOX!\"\"Um, are you okay?\" Matt asked snickering.\"Yes.\" Scott grunted.\"Sure you are. Need some help?\" He teased.\"No. I can do it myself.\"\"You sureeee?\"\"Yes.\"\"Positive?\"\"Yup.\"\"Certain?\" he persisted\"Yes! I'm damn sure!\"\"Whatever you say, Scott.\"\"Shut up, Matthew.\" Scott grumbledMatt muttered nonchalant words under his breath while turning to the door.\"Wait! Err… can you read me the instructions?\"\"Sure,\" he said.He wanted to say \"Does little Scottie not know how to make a bed?\" But he decided against it, they weren't really fighting. No need to ruin it. Although the thought of calling Scott; strong, quiet, tough Scott 'Scottie' made him laugh out loud.And though Scott gave him a strange look, it seemed he hadn't heard what Matt had thought or if he had he didn't care.\"Pass me the screw driver would you? \"Scott huffed, under the weight of the headboard.\"Mm-hm.\" He grasped the heavy metal machine in his hands and tossed the screw driver at Scott's head.I tossed the screw driver at him. Matt tried to figure out what was wrong with that sentence.I threw a metal tool at Scott's head…SHIT!The short haired twin was holding the headboard with both hands- and struggling with it at that. It was a heavy meter long and two inch thick piece of wood. There was no way he could stop the screw driver from slamming into his skull. Matt didn't know what to do until suddenly an idea hit him. No pun intended.He stopped the tool with his powers right before it collided with the American's head. And laid it at the Scott's feet, he then held up the headboard with his powers for the other boy to connect it to the rails.\"Thanks.\" The Scott mumbled trying to suppress a smile. He didn't want to show Matt how much he appreciated his help. He wanted the other boy to think he could manage on his own.\"Anytime.\" Matt breathed.They finished the beds rather quickly, it was like having ten people rather than two. They finished both beds in an hour and by then Scott was dying from the Peruvian heat.\"Holy crap… it's so damn hot!\" Scott moaned, taking off his shirt, unintentionally showing off a considerable amount of his attractive body. Poor Matt had to strain to keep his eyes off of Scott's hot, sweaty body. He licked his lips and tried to think of other things.Hot dogs, hot dogs, hot dogs… He tried, but it only made it worse; Matt began thinking of what was under the clothing Scott had left on.Shit! Um… what's something not dick shaped…? He looked to Scott to see if he had telepathically heard his problem, Luckily, Scott was too caught up on the heat to notice.Pencil sharpeners… oh shit pencils go in the hole… Matt began to feel aroused.You're kidding! Matt thought as he got up and sprinted* to the washroom.Am I really that sex deprived? Damn hormones.Matt didn't know how to calm his organ. He tried to think of a turn off.He tried to imagine his old foster parent Jayne Deverill in lingerie but not only did it not work it just made Matt want to puke. Maybe a shower would help. He made sure there was a towel before he stepped into the shower.Am I really that sex deprived? Damn hormones.Those words floated into Scott's mind as he went into Matt's mind to see where he had disappeared to. He saw himself shirtless and he felt a weird tingling in his man parts as he put himself in Matt's mind frame.He flung himself back into reality. Did I… Holy fuck. Did I turn on Matt, the first of the five, the good-looking, blue-eyed English boy? Scott lay in the bed they had just put together, thinking about what to do. He didn't know why, but for some reason he was happy at the thought of Matt being aroused because of him. He smiled involuntarily. He heard the shower shut off and two feet step onto the bathroom floor.I could always ask him, but what would I say, \"Hey Matt, are you turned on by me?\" And let's say I asked that what would he say, \"Yeah I got really hard.\" No I don't think so.Just acting like I never heard it would be best.\"Um, Scott?\" Matt's voice rang in Scott's ears snapping him out of his thoughts, \"Yeah?\" he called back.\"I… I forgot clothes!\" Matt called, clearly embarrassed.\"No worries.\" Scott chuckled, walking into Matt's room; Scott grabbed a blue T-shirt and some jeans. He began to make his way back to the washroom when he realized, boxers, and Matt needs boxers.He opened the top drawer in Matthew's bedside table in it was not underwear but a small book.Something was pulling him to the book, just a peek… he thought opening it.And he has the most amazing eyes I have EVER seen I can't even try to describe them…Who? Who has amazing eyes? Scott thought skimming through the book.One line in particular caught his attention.Why can't I like a female… and of ALL guys why did I have to fall for Scott? Stupid heart…This shocked Scott more than… finding out he was one of the five! He'd never expected Matt to be so vulnerable; he didn't expect Matt to love him either!I do not like the whole unexpected thing.Another thing he didn't expect was Matt walking in on him invading his privacy in only a towel.\"I was looking for your boxers!\" He lied.\"Of course you were.\" Matt replied, expressionless.\"Matt, I didn't mean to... I just-\"\"Don't worry about it.\" Matt cut him off. \"I should've told you anyway, I just… I tried. But I never did.\"\"Why not?\" Scott asked, it slipped out. The look on Matt was astonished.I'm an idiot! Such an idiot…\"What was that?\" Matt mumbled.\"I said, why not?\" Scott repeated trying to sound sure of himself.\"Why didn't I tell you?\"\"Yeah.\"\"Um because I was… nervous.\"\"Nervous..?\"\"Nervous.\"\"You're Matt fucking Freeman, first of the five! How are you nervous because of this?\" Scott cried.\"Well it's not my fault!\"\"How isn't it!\"\"It's your fault for being so,\" Matt gestured at Scott's entire body \"you-ish!\"\"Wow, very manly Matt.\"\"Shut up!\"Scott started to make his way over to Matt so that he could hurt him, \"Shut up? Really?\"\"Yeah! You got a problem with that?\" Matt said, also walking towards Scott.\"I do! As a matter of fact\"Both of their faces were screwed up in anger.He even looks hot when he's mad… Scott heard Matt think.Scott brought his hand up to Matt's face as if he were about to punch him; instead he cupped the blue-eyed boys face and began to kiss him.Matt pulled away as quickly as possible, \"Scott..?\"Sadness washed over Scott like never before.\"Sorry I just thought that…\" he trailed off.\"I was just wondering why, Scott. I didn't… dislike it.\" Matt smiled hesitantly.\"Why didn't you just say so?\" Scott smirked pulling Matt closer to him. He had no idea why he was doing this but he liked it.Their lips sealed once more and Scott was sent spinning into pure bliss, he pushed Matt onto the mattress behind him.Underneath him Matt moved up and down; Scott loved the way Matt's body was rubbing against him, he loved the way Matt's black hair felt clenched in his fists, honestly he was loving every breathtaking second of this.\"Scott?\" Matt mumbled into Scott's lips.\"Hmm?\"\"When are the others getting back?\"\"Mmm,\" Scott stopped kissing Matt to look into Jamie's mind; \"Um, not till' tomorrow\" He stated smirking. They both knew where this was going, with only a towel covering mat and shorts for Scott this would be a pleasurable night.A/N: Sorry it took so long, I've been sick and now I know the diary is kind of un-Mattish but too bad x D I'm up for comments, thanks to BLC for editing the first half, and the second half was written at 4am so give me some credit :D hah. I'll try to update soon. I'm going to do a second half of this or maybe more.  so don't think I'm leaving it at that. Hehe.Xoxoxo ThatGirl100Ps. Booked it means sprint.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id = random.randint(1, 100)\n",
    "print(labels[id])\n",
    "print(stories[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    print(n, d)\n",
    "    i = 0\n",
    "    for line in fin:\n",
    "        if i == VOCAB_SIZE:\n",
    "            break\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "        i += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab():\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    word_vectors = pkl.load(open(\"fasttext_word_vectors.p\", \"rb\"))\n",
    "    id2token = list(word_vectors.keys())\n",
    "    token2id = dict(zip(word_vectors, range(2,2+len(word_vectors)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return word_vectors, token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre2class = dict(zip(genre_categories, range(len(genre_categories))))\n",
    "class2genre = list(genre_categories)\n",
    "\n",
    "def convert_genres_to_integers(genre):\n",
    "    return genre2class[genre]\n",
    "\n",
    "def verify_order(stories, labels):\n",
    "    i = random.randint(1, len(stories))\n",
    "    print(class2genre[labels[i]])\n",
    "    print(\"\\n\")\n",
    "    print(stories[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenize each entry in a list of sentences\n",
    "def tokenize_story(story):\n",
    "    sentence_list = nltk.sent_tokenize(story)\n",
    "    words = [] \n",
    "    for i in range(len(sentence_list)):\n",
    "            words = words + word_tokenize(sentence_list[i])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"one-hot encode\": convert each token to id in vocabulary vector (token2id)\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vocabulary & embedding matrix from FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors, token2id, id2token = build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50002, 300)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_weights = np.array(list(word_vectors.values()))\n",
    "pad_vec = np.zeros((1, 300))\n",
    "unk_vec = np.random.randn(1, 300) * 0.01\n",
    "pad_unk_vecs = np.vstack((pad_vec, unk_vec))\n",
    "_WEIGHTS = np.vstack((pad_unk_vecs, _weights))\n",
    "_WEIGHTS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('_WEIGHTS.pkl', 'rb') as pickle_file:\n",
    "    _WEIGHTS = pkl.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data for the models\n",
    "#### Shuffle, word tokenize, one-hot index into vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline1(stories, labels):\n",
    "    tokenized = []\n",
    "    i = 0\n",
    "    for story in stories:\n",
    "        s = tokenize_story(story)\n",
    "        tokenized.append(s) \n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "    print(i)\n",
    "    print(\"done!\")\n",
    "    stories_indices = token2index_dataset(tokenized)\n",
    "    return stories_indices, labels\n",
    "    \n",
    "            \n",
    "\n",
    "def data_pipeline(stories, genre_labels, verify=True):\n",
    "    labels = []\n",
    "    for g in genre_labels:\n",
    "        labels.append(convert_genres_to_integers(g))\n",
    "        \n",
    "    seed = random.randint(1, 100)\n",
    "    print(\"Random seed for shuffling: {}\".format(seed))\n",
    "    random.Random(seed).shuffle(stories)\n",
    "    random.Random(seed).shuffle(labels)\n",
    "    \n",
    "    print(\"\\nVerifying that the data and label match after shuffling\")\n",
    "    if verify:\n",
    "        verify_order(stories, labels)\n",
    "        \n",
    "    print(\"\\nTokenizing stories...\")  \n",
    "    tokenized = []\n",
    "    truth_labels = []\n",
    "    i = 0\n",
    "    for story in stories:\n",
    "        if isinstance(story, str):\n",
    "            s = tokenize_story(story)\n",
    "            tokenized.append(s)\n",
    "            truth_labels.append(labels[i])\n",
    "        i += 1\n",
    "    print(\"done!\")\n",
    "    \n",
    "    print(len(tokenized))\n",
    "    print(len(truth_labels))\n",
    "    \n",
    "    print(\"\\nOne-hot encoding words (converting words to ids, corresponding to vocabulary)\")  \n",
    "    stories_indices = token2index_dataset(tokenized)\n",
    "    print(\"done!\")\n",
    "    \n",
    "    return (stories_indices, truth_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class StoriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stories_data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param sent1_data_list: list of sentence1's (index matches sentence2's and target_list below)\n",
    "        @param target_list: list of correct labels\n",
    "        \"\"\"\n",
    "        self.stories_data_list = stories_data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.stories_data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stories_data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        ###\n",
    "        ### Returns [[story 1 tokens..], [story 2 tokens..]]\n",
    "        ###\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        story_tokens_idx = self.stories_data_list[key][:MAX_STORY_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [story_tokens_idx, label]\n",
    "\n",
    "def stories_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Custom function that dynamically pads the batch so that all data have the same length\n",
    "    \"\"\"\n",
    "    stories_data_list = []\n",
    "    stories_length_list = []\n",
    "    label_list = []\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[1])\n",
    "        stories_length_list.append(len(datum[0]))\n",
    "        padded_vec_1 = np.pad(np.array(datum[0]), pad_width=((0,MAX_STORY_LENGTH-len(datum[0]))), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        stories_data_list.append(padded_vec_1)\n",
    "    return [torch.from_numpy(np.array(stories_data_list)), \n",
    "            torch.LongTensor(stories_length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        3999\n",
       "unique          3\n",
       "top       Romance\n",
       "freq         3453\n",
       "Name: genre1, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['genre1'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6840\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# stories = list(content['story'])\n",
    "# labels = list(content['genre1'])\n",
    "\n",
    "# train_stories_indices, train_labels = data_pipeline(stories, labels)\n",
    "\n",
    "train_stories_indices, train_labels = pipeline1(stories, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating train_loader.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = StoriesDataset(train_stories_indices, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=stories_collate_func,\n",
    "                                           #shuffle=True\n",
    "                                          )\n",
    "print(\"Finished creating train_loader.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_dataset.pkl', 'wb') as pickle_file:\n",
    "    pkl.dump(train_dataset, pickle_file)\n",
    "with open('train_loader.pkl', 'wb') as pickle_file:\n",
    "    pkl.dump(train_loader, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_stories_indices.pkl', 'rb') as pickle_file:\n",
    "    train_stories_indices = pkl.load(pickle_file)\n",
    "    \n",
    "with open('train_labels.pkl', 'rb') as pickle_file:\n",
    "    train_labels = pkl.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, embedding, embedding_size, hidden_size, num_layers, num_classes):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(2*hidden_size, 100)\n",
    "        self.linear2 = nn.Linear(100, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(2*self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, data, lengths):\n",
    "        \n",
    "        batch_size = data.size()[0]\n",
    "        lengths = list(lengths)\n",
    "        \n",
    "        reverse_sorted_indices = [x for _, x in sorted(zip(lengths, range(len(lengths))), reverse=True)]\n",
    "        reverse_sorted_lengths = [x for x, _ in sorted(zip(lengths, range(len(lengths))), reverse=True)]\n",
    "        reverse_sorted_lengths = np.array(reverse_sorted_lengths)\n",
    "        \n",
    "        stories = data.to(device)\n",
    "        reverse_sorted_data = torch.index_select(stories, 0, torch.tensor(reverse_sorted_indices).to(device))\n",
    "        \n",
    "        # get embedding\n",
    "        embed = self.embedding(reverse_sorted_data)\n",
    "        \n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # pack padded sequence\n",
    "        embed = torch.nn.utils.rnn.pack_padded_sequence(embed, reverse_sorted_lengths, batch_first=True)\n",
    "        \n",
    "        # fprop though RNN\n",
    "        gru_out, self.hidden = self.gru(embed, self.hidden)\n",
    "                \n",
    "        ### MATCHING BACK\n",
    "        change_it_back = [x for _, x in sorted(zip(reverse_sorted_indices, range(len(reverse_sorted_indices))))]\n",
    "        self.hidden = torch.index_select(self.hidden, 1, torch.LongTensor(change_it_back).to(device)) \n",
    "        \n",
    "        self.hidden = torch.cat([self.hidden[0, :, :], self.hidden[1, :, :]], dim=1)\n",
    "        # 8 by 512\n",
    "        \n",
    "        # should be 32 by 256\n",
    "        linear1 = self.linear1(self.hidden)\n",
    "        # should be 32 by num_classes\n",
    "        linear1 = F.relu(linear1.contiguous().view(-1, linear1.size(-1))).view(linear1.shape) \n",
    "        logits = self.linear2(linear1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Test the model's performance on a dataset\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for (data, lengths, labels) in loader:\n",
    "        data_batch, length_batch, label_batch = data.to(device), lengths.to(device), labels.to(device)\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        total += label_batch.size(0)\n",
    "        correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def train_model(model, lr = 0.001, num_epochs = 7, criterion = nn.CrossEntropyLoss()):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "    max_val_acc = 0\n",
    "    losses = []\n",
    "    xs = 0\n",
    "    val_accs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data.to(device), lengths.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i > 0 and i % 10 == 0:\n",
    "                print(loss)\n",
    "                train_acc = test_model(train_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Train Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), train_acc))\n",
    "            # validate every 100 iterations\n",
    "#             if i > 0 and i % 100 == 0:\n",
    "#                 # validate\n",
    "#                 val_acc = test_model(val_loader, model)\n",
    "#                 val_accs.append(val_acc)\n",
    "#                 xs += 100\n",
    "#                 if val_acc > max_val_acc:\n",
    "#                     max_val_acc = val_acc\n",
    "#                 print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "#                            epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "#                 print('Epoch: [{}/{}], Step: [{}/{}], Training Loss: {}'.format( \n",
    "#                            epoch+1, num_epochs, i+1, len(train_loader), loss))\n",
    "                \n",
    "#     print(\"Max Validation Accuracy: {}\".format(max_val_acc))\n",
    "    return max_val_acc, losses, xs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [11/855], Train Acc: 49.985380116959064\n",
      "tensor(0.6553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [21/855], Train Acc: 53.099415204678365\n",
      "tensor(0.7941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [31/855], Train Acc: 53.58187134502924\n",
      "tensor(0.7130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [41/855], Train Acc: 57.00292397660819\n",
      "tensor(0.7058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [51/855], Train Acc: 54.91228070175438\n",
      "tensor(0.7328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [61/855], Train Acc: 50.74561403508772\n",
      "tensor(0.5865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [71/855], Train Acc: 57.280701754385966\n",
      "tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [81/855], Train Acc: 58.99122807017544\n",
      "tensor(0.7603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [91/855], Train Acc: 61.345029239766085\n",
      "tensor(0.6227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [101/855], Train Acc: 60.07309941520468\n",
      "tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [111/855], Train Acc: 60.51169590643275\n",
      "tensor(0.6565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [121/855], Train Acc: 59.69298245614035\n",
      "tensor(1.7132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [131/855], Train Acc: 60.73099415204678\n",
      "tensor(0.7024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [141/855], Train Acc: 60.058479532163744\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [151/855], Train Acc: 60.30701754385965\n",
      "tensor(0.7587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [161/855], Train Acc: 60.86257309941521\n",
      "tensor(0.8230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [171/855], Train Acc: 60.90643274853801\n",
      "tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [181/855], Train Acc: 62.42690058479532\n",
      "tensor(0.6427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [191/855], Train Acc: 58.39181286549707\n",
      "tensor(0.6418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [201/855], Train Acc: 61.301169590643276\n",
      "tensor(0.6331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [211/855], Train Acc: 62.63157894736842\n",
      "tensor(0.5519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [221/855], Train Acc: 63.23099415204678\n",
      "tensor(0.5104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [231/855], Train Acc: 64.21052631578948\n",
      "tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [241/855], Train Acc: 63.47953216374269\n",
      "tensor(0.6825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [251/855], Train Acc: 63.932748538011694\n",
      "tensor(0.4468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [261/855], Train Acc: 63.34795321637427\n",
      "tensor(0.6152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [271/855], Train Acc: 62.83625730994152\n",
      "tensor(0.6451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [281/855], Train Acc: 62.792397660818715\n",
      "tensor(0.5928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [291/855], Train Acc: 62.719298245614034\n",
      "tensor(0.6018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [301/855], Train Acc: 62.76315789473684\n",
      "tensor(0.8163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [311/855], Train Acc: 63.08479532163743\n",
      "tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [321/855], Train Acc: 58.771929824561404\n",
      "tensor(0.6254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [331/855], Train Acc: 63.611111111111114\n",
      "tensor(0.5169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [341/855], Train Acc: 62.63157894736842\n",
      "tensor(0.9532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [351/855], Train Acc: 63.27485380116959\n",
      "tensor(0.6784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [361/855], Train Acc: 62.92397660818713\n",
      "tensor(0.5702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [371/855], Train Acc: 62.57309941520468\n",
      "tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [381/855], Train Acc: 62.33918128654971\n",
      "tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [391/855], Train Acc: 63.58187134502924\n",
      "tensor(0.6576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [401/855], Train Acc: 63.728070175438596\n",
      "tensor(0.5980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [411/855], Train Acc: 63.75730994152047\n",
      "tensor(0.7667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [421/855], Train Acc: 62.17836257309941\n",
      "tensor(0.6811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [431/855], Train Acc: 60.95029239766082\n",
      "tensor(0.6119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [441/855], Train Acc: 60.84795321637427\n",
      "tensor(0.5801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [451/855], Train Acc: 60.994152046783626\n",
      "tensor(0.7388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [461/855], Train Acc: 61.09649122807018\n",
      "tensor(0.6472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [471/855], Train Acc: 60.760233918128655\n",
      "tensor(0.6325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [481/855], Train Acc: 61.16959064327485\n",
      "tensor(0.6580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [491/855], Train Acc: 61.40350877192982\n",
      "tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [501/855], Train Acc: 61.345029239766085\n",
      "tensor(0.6219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [511/855], Train Acc: 61.59356725146199\n",
      "tensor(0.5148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [521/855], Train Acc: 60.833333333333336\n",
      "tensor(0.5589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [531/855], Train Acc: 61.18421052631579\n",
      "tensor(0.5095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [541/855], Train Acc: 60.90643274853801\n",
      "tensor(0.5168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [551/855], Train Acc: 62.134502923976605\n",
      "tensor(0.6167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [561/855], Train Acc: 63.40643274853801\n",
      "tensor(0.5734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [571/855], Train Acc: 61.37426900584796\n",
      "tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [581/855], Train Acc: 57.52923976608187\n",
      "tensor(0.6236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [591/855], Train Acc: 58.9766081871345\n",
      "tensor(0.5229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [601/855], Train Acc: 60.87719298245614\n",
      "tensor(0.6063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [611/855], Train Acc: 61.56432748538012\n",
      "tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [621/855], Train Acc: 61.461988304093566\n",
      "tensor(0.5677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [631/855], Train Acc: 61.44736842105263\n",
      "tensor(0.7542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [641/855], Train Acc: 61.79824561403509\n",
      "tensor(0.6581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [651/855], Train Acc: 63.04093567251462\n",
      "tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [661/855], Train Acc: 62.99707602339181\n",
      "tensor(0.5907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [671/855], Train Acc: 62.953216374269005\n",
      "tensor(0.6121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [681/855], Train Acc: 63.78654970760234\n",
      "tensor(0.8048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [691/855], Train Acc: 64.09356725146199\n",
      "tensor(0.7704, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [701/855], Train Acc: 64.34210526315789\n",
      "tensor(0.7737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [711/855], Train Acc: 65.13157894736842\n",
      "tensor(0.6196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [721/855], Train Acc: 64.91228070175438\n",
      "tensor(0.6266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [731/855], Train Acc: 63.01169590643275\n",
      "tensor(0.6268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [741/855], Train Acc: 63.04093567251462\n",
      "tensor(0.6199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [751/855], Train Acc: 63.91812865497076\n",
      "tensor(0.6280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [761/855], Train Acc: 64.63450292397661\n",
      "tensor(0.4915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [771/855], Train Acc: 64.57602339181287\n",
      "tensor(0.5628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [781/855], Train Acc: 63.932748538011694\n",
      "tensor(0.5825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [791/855], Train Acc: 63.026315789473685\n",
      "tensor(0.5378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [801/855], Train Acc: 63.34795321637427\n",
      "tensor(0.5903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [811/855], Train Acc: 64.06432748538012\n",
      "tensor(0.7439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [821/855], Train Acc: 65.453216374269\n",
      "tensor(0.6986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [831/855], Train Acc: 64.89766081871345\n",
      "tensor(0.7396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [841/855], Train Acc: 59.57602339181287\n",
      "tensor(0.5268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [1/5], Step: [851/855], Train Acc: 59.50292397660819\n",
      "tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [11/855], Train Acc: 61.038011695906434\n",
      "tensor(0.6835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [21/855], Train Acc: 61.345029239766085\n",
      "tensor(0.7206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [31/855], Train Acc: 61.72514619883041\n",
      "tensor(0.7699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [41/855], Train Acc: 61.739766081871345\n",
      "tensor(0.7112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [51/855], Train Acc: 64.23976608187135\n",
      "tensor(0.6458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [61/855], Train Acc: 64.40058479532163\n",
      "tensor(0.5690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [71/855], Train Acc: 65.19005847953217\n",
      "tensor(0.6335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [81/855], Train Acc: 64.8391812865497\n",
      "tensor(0.5795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [91/855], Train Acc: 64.40058479532163\n",
      "tensor(0.5203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [101/855], Train Acc: 64.98538011695906\n",
      "tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [111/855], Train Acc: 64.73684210526316\n",
      "tensor(0.6403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [121/855], Train Acc: 63.50877192982456\n",
      "tensor(0.6244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [131/855], Train Acc: 64.60526315789474\n",
      "tensor(0.6770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [141/855], Train Acc: 66.15497076023392\n",
      "tensor(0.7726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [151/855], Train Acc: 62.07602339181287\n",
      "tensor(0.6814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [161/855], Train Acc: 66.6374269005848\n",
      "tensor(0.8476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [171/855], Train Acc: 64.57602339181287\n",
      "tensor(0.7650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [181/855], Train Acc: 63.24561403508772\n",
      "tensor(0.6437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [191/855], Train Acc: 65.21929824561404\n",
      "tensor(0.6389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [201/855], Train Acc: 64.4298245614035\n",
      "tensor(0.5826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [211/855], Train Acc: 66.3157894736842\n",
      "tensor(0.4545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [221/855], Train Acc: 66.15497076023392\n",
      "tensor(0.5755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [231/855], Train Acc: 64.41520467836257\n",
      "tensor(0.4912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [241/855], Train Acc: 65.6140350877193\n",
      "tensor(0.5097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [251/855], Train Acc: 65.71637426900585\n",
      "tensor(0.3354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [261/855], Train Acc: 64.44444444444444\n",
      "tensor(0.6519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [271/855], Train Acc: 66.14035087719299\n",
      "tensor(0.5809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [281/855], Train Acc: 63.026315789473685\n",
      "tensor(0.5615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [291/855], Train Acc: 64.94152046783626\n",
      "tensor(0.6233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [301/855], Train Acc: 64.50292397660819\n",
      "tensor(0.7904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [311/855], Train Acc: 65.71637426900585\n",
      "tensor(0.6082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [321/855], Train Acc: 61.62280701754386\n",
      "tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [331/855], Train Acc: 65.11695906432749\n",
      "tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [341/855], Train Acc: 66.59356725146199\n",
      "tensor(0.8291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [351/855], Train Acc: 65.89181286549707\n",
      "tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [361/855], Train Acc: 65.93567251461988\n",
      "tensor(0.4921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [371/855], Train Acc: 59.83918128654971\n",
      "tensor(0.5174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [381/855], Train Acc: 65.01461988304094\n",
      "tensor(0.6428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [391/855], Train Acc: 65.24853801169591\n",
      "tensor(0.7879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [401/855], Train Acc: 64.18128654970761\n",
      "tensor(0.5867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [411/855], Train Acc: 64.31286549707602\n",
      "tensor(0.7106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [421/855], Train Acc: 64.19590643274854\n",
      "tensor(0.7962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [431/855], Train Acc: 64.91228070175438\n",
      "tensor(0.6087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [441/855], Train Acc: 64.94152046783626\n",
      "tensor(0.5534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [451/855], Train Acc: 65.58479532163743\n",
      "tensor(0.7661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [461/855], Train Acc: 65.55555555555556\n",
      "tensor(0.6576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [471/855], Train Acc: 65.38011695906432\n",
      "tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [481/855], Train Acc: 66.6374269005848\n",
      "tensor(0.6305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [491/855], Train Acc: 66.81286549707602\n",
      "tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [501/855], Train Acc: 66.0233918128655\n",
      "tensor(0.6325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [511/855], Train Acc: 66.34502923976608\n",
      "tensor(0.4674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [521/855], Train Acc: 65.83333333333333\n",
      "tensor(0.5206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [531/855], Train Acc: 66.16959064327486\n",
      "tensor(0.4955, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2/5], Step: [541/855], Train Acc: 65.33625730994152\n",
      "tensor(0.5360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [551/855], Train Acc: 66.54970760233918\n",
      "tensor(0.6257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [561/855], Train Acc: 66.71052631578948\n",
      "tensor(0.5681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [571/855], Train Acc: 63.39181286549707\n",
      "tensor(0.7482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [581/855], Train Acc: 61.198830409356724\n",
      "tensor(0.5906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [591/855], Train Acc: 60.64327485380117\n",
      "tensor(0.4694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [601/855], Train Acc: 64.78070175438596\n",
      "tensor(0.5426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [611/855], Train Acc: 66.56432748538012\n",
      "tensor(0.5860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [621/855], Train Acc: 66.16959064327486\n",
      "tensor(0.5033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [631/855], Train Acc: 65.87719298245614\n",
      "tensor(0.7925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [641/855], Train Acc: 65.92105263157895\n",
      "tensor(0.7492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [651/855], Train Acc: 67.11988304093568\n",
      "tensor(0.5554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [661/855], Train Acc: 66.6374269005848\n",
      "tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [671/855], Train Acc: 66.30116959064327\n",
      "tensor(0.5960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [681/855], Train Acc: 67.19298245614036\n",
      "tensor(0.7939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [691/855], Train Acc: 67.32456140350877\n",
      "tensor(0.7171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [701/855], Train Acc: 67.54385964912281\n",
      "tensor(0.6798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [711/855], Train Acc: 67.63157894736842\n",
      "tensor(0.7424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [721/855], Train Acc: 66.0233918128655\n",
      "tensor(0.6272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [731/855], Train Acc: 67.92397660818713\n",
      "tensor(0.5468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [741/855], Train Acc: 68.0701754385965\n",
      "tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [751/855], Train Acc: 67.35380116959064\n",
      "tensor(0.5378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [761/855], Train Acc: 68.26023391812865\n",
      "tensor(0.4150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [771/855], Train Acc: 68.15789473684211\n",
      "tensor(0.4667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [781/855], Train Acc: 66.59356725146199\n",
      "tensor(0.4858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [791/855], Train Acc: 66.53508771929825\n",
      "tensor(0.5677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [801/855], Train Acc: 67.48538011695906\n",
      "tensor(0.5517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [811/855], Train Acc: 67.41228070175438\n",
      "tensor(0.7202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [821/855], Train Acc: 66.81286549707602\n",
      "tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [831/855], Train Acc: 67.45614035087719\n",
      "tensor(0.5955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [841/855], Train Acc: 67.11988304093568\n",
      "tensor(0.4956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [2/5], Step: [851/855], Train Acc: 67.54385964912281\n",
      "tensor(0.8600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [11/855], Train Acc: 66.98830409356725\n",
      "tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [21/855], Train Acc: 66.30116959064327\n",
      "tensor(0.8241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [31/855], Train Acc: 66.35964912280701\n",
      "tensor(0.5811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [41/855], Train Acc: 65.89181286549707\n",
      "tensor(0.7246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [51/855], Train Acc: 67.45614035087719\n",
      "tensor(0.5811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [61/855], Train Acc: 67.51461988304094\n",
      "tensor(0.5893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [71/855], Train Acc: 67.42690058479532\n",
      "tensor(0.6750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [81/855], Train Acc: 68.17251461988305\n",
      "tensor(0.5119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [91/855], Train Acc: 67.19298245614036\n",
      "tensor(0.4923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [101/855], Train Acc: 68.8157894736842\n",
      "tensor(0.6586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [111/855], Train Acc: 67.61695906432749\n",
      "tensor(0.5951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [121/855], Train Acc: 66.68128654970761\n",
      "tensor(0.5955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [131/855], Train Acc: 66.84210526315789\n",
      "tensor(0.6354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [141/855], Train Acc: 67.71929824561404\n",
      "tensor(0.7373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [151/855], Train Acc: 65.99415204678363\n",
      "tensor(0.6437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [161/855], Train Acc: 68.7719298245614\n",
      "tensor(0.8166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [171/855], Train Acc: 66.60818713450293\n",
      "tensor(0.6804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [181/855], Train Acc: 67.54385964912281\n",
      "tensor(0.7178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [191/855], Train Acc: 64.21052631578948\n",
      "tensor(0.5998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [201/855], Train Acc: 68.49415204678363\n",
      "tensor(0.5834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [211/855], Train Acc: 68.40643274853801\n",
      "tensor(0.4127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [221/855], Train Acc: 66.82748538011695\n",
      "tensor(0.5429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [231/855], Train Acc: 67.69005847953217\n",
      "tensor(0.6887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [241/855], Train Acc: 68.84502923976608\n",
      "tensor(0.4498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [251/855], Train Acc: 66.98830409356725\n",
      "tensor(0.2958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [261/855], Train Acc: 67.00292397660819\n",
      "tensor(0.7169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [271/855], Train Acc: 67.51461988304094\n",
      "tensor(0.5449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [281/855], Train Acc: 67.38304093567251\n",
      "tensor(0.5390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [291/855], Train Acc: 65.78947368421052\n",
      "tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [301/855], Train Acc: 66.44736842105263\n",
      "tensor(0.8540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [311/855], Train Acc: 67.63157894736842\n",
      "tensor(0.5044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [321/855], Train Acc: 65.70175438596492\n",
      "tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [331/855], Train Acc: 67.44152046783626\n",
      "tensor(0.4471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [341/855], Train Acc: 68.53801169590643\n",
      "tensor(0.7010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [351/855], Train Acc: 68.02631578947368\n",
      "tensor(0.5822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [361/855], Train Acc: 67.61695906432749\n",
      "tensor(0.5107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [371/855], Train Acc: 63.9766081871345\n",
      "tensor(0.4415, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3/5], Step: [381/855], Train Acc: 64.69298245614036\n",
      "tensor(0.6209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [391/855], Train Acc: 67.35380116959064\n",
      "tensor(0.9022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [401/855], Train Acc: 67.98245614035088\n",
      "tensor(0.6446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [411/855], Train Acc: 68.5233918128655\n",
      "tensor(0.6439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [421/855], Train Acc: 67.11988304093568\n",
      "tensor(0.7591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [431/855], Train Acc: 67.16374269005848\n",
      "tensor(0.6143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [441/855], Train Acc: 66.97368421052632\n",
      "tensor(0.5172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [451/855], Train Acc: 66.71052631578948\n",
      "tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [461/855], Train Acc: 67.3391812865497\n",
      "tensor(0.6707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [471/855], Train Acc: 67.57309941520468\n",
      "tensor(0.6275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [481/855], Train Acc: 68.40643274853801\n",
      "tensor(0.6075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [491/855], Train Acc: 68.31871345029239\n",
      "tensor(0.6639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [501/855], Train Acc: 65.74561403508773\n",
      "tensor(0.5730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [511/855], Train Acc: 65.96491228070175\n",
      "tensor(0.4479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [521/855], Train Acc: 67.69005847953217\n",
      "tensor(0.5103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [531/855], Train Acc: 68.8157894736842\n",
      "tensor(0.5134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [541/855], Train Acc: 67.9093567251462\n",
      "tensor(0.5397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [551/855], Train Acc: 67.79239766081871\n",
      "tensor(0.6752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [561/855], Train Acc: 68.26023391812865\n",
      "tensor(0.5840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [571/855], Train Acc: 65.99415204678363\n",
      "tensor(0.6829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [581/855], Train Acc: 66.28654970760233\n",
      "tensor(0.5751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [591/855], Train Acc: 66.30116959064327\n",
      "tensor(0.4031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [601/855], Train Acc: 67.953216374269\n",
      "tensor(0.5644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [611/855], Train Acc: 68.42105263157895\n",
      "tensor(0.5990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [621/855], Train Acc: 67.14912280701755\n",
      "tensor(0.4525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [631/855], Train Acc: 67.38304093567251\n",
      "tensor(0.8455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [641/855], Train Acc: 68.17251461988305\n",
      "tensor(0.6829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [651/855], Train Acc: 67.5\n",
      "tensor(0.5530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [661/855], Train Acc: 68.46491228070175\n",
      "tensor(0.5998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [671/855], Train Acc: 68.15789473684211\n",
      "tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [681/855], Train Acc: 62.909356725146196\n",
      "tensor(0.8611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [691/855], Train Acc: 68.78654970760233\n",
      "tensor(0.6810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [701/855], Train Acc: 68.27485380116958\n",
      "tensor(0.7452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [711/855], Train Acc: 68.83040935672514\n",
      "tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [721/855], Train Acc: 66.97368421052632\n",
      "tensor(0.6315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [731/855], Train Acc: 68.96198830409357\n",
      "tensor(0.4895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [741/855], Train Acc: 69.31286549707602\n",
      "tensor(0.4797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [751/855], Train Acc: 69.07894736842105\n",
      "tensor(0.5141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [761/855], Train Acc: 68.84502923976608\n",
      "tensor(0.3993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [771/855], Train Acc: 68.9766081871345\n",
      "tensor(0.4594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [781/855], Train Acc: 68.99122807017544\n",
      "tensor(0.4378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [791/855], Train Acc: 68.85964912280701\n",
      "tensor(0.5957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [801/855], Train Acc: 69.53216374269006\n",
      "tensor(0.5129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [811/855], Train Acc: 68.53801169590643\n",
      "tensor(0.6200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [821/855], Train Acc: 68.53801169590643\n",
      "tensor(0.6341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [831/855], Train Acc: 68.3625730994152\n",
      "tensor(0.6010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [841/855], Train Acc: 68.02631578947368\n",
      "tensor(0.4580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [3/5], Step: [851/855], Train Acc: 68.20175438596492\n",
      "tensor(0.8408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [11/855], Train Acc: 68.27485380116958\n",
      "tensor(0.7405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [21/855], Train Acc: 66.66666666666667\n",
      "tensor(0.7994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [31/855], Train Acc: 67.32456140350877\n",
      "tensor(0.5457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [41/855], Train Acc: 66.95906432748538\n",
      "tensor(0.6385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [51/855], Train Acc: 69.40058479532163\n",
      "tensor(0.5356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [61/855], Train Acc: 68.53801169590643\n",
      "tensor(0.5212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [71/855], Train Acc: 68.55263157894737\n",
      "tensor(0.6344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [81/855], Train Acc: 70.21929824561404\n",
      "tensor(0.5163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [91/855], Train Acc: 68.20175438596492\n",
      "tensor(0.4201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [101/855], Train Acc: 70.39473684210526\n",
      "tensor(0.5792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [111/855], Train Acc: 69.41520467836257\n",
      "tensor(0.5862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [121/855], Train Acc: 68.46491228070175\n",
      "tensor(0.5992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [131/855], Train Acc: 69.09356725146199\n",
      "tensor(0.6649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [141/855], Train Acc: 68.78654970760233\n",
      "tensor(0.7844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [151/855], Train Acc: 66.34502923976608\n",
      "tensor(0.5994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [161/855], Train Acc: 70.14619883040936\n",
      "tensor(0.7672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [171/855], Train Acc: 67.953216374269\n",
      "tensor(0.6817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [181/855], Train Acc: 67.5\n",
      "tensor(0.7959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [191/855], Train Acc: 64.37134502923976\n",
      "tensor(0.5527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [201/855], Train Acc: 66.97368421052632\n",
      "tensor(0.5971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [211/855], Train Acc: 65.80409356725146\n",
      "tensor(0.4609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [221/855], Train Acc: 67.96783625730994\n",
      "tensor(0.5200, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4/5], Step: [231/855], Train Acc: 67.3391812865497\n",
      "tensor(0.7292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [241/855], Train Acc: 66.1842105263158\n",
      "tensor(0.4680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [251/855], Train Acc: 65.71637426900585\n",
      "tensor(0.4864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [261/855], Train Acc: 66.6374269005848\n",
      "tensor(0.6585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [271/855], Train Acc: 68.1140350877193\n",
      "tensor(0.5253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [281/855], Train Acc: 65.46783625730994\n",
      "tensor(0.4766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [291/855], Train Acc: 69.09356725146199\n",
      "tensor(0.6719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [301/855], Train Acc: 66.35964912280701\n",
      "tensor(0.8353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [311/855], Train Acc: 68.46491228070175\n",
      "tensor(0.5354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [321/855], Train Acc: 64.37134502923976\n",
      "tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [331/855], Train Acc: 68.21637426900585\n",
      "tensor(0.4047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [341/855], Train Acc: 68.14327485380117\n",
      "tensor(0.5961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [351/855], Train Acc: 68.30409356725146\n",
      "tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [361/855], Train Acc: 67.96783625730994\n",
      "tensor(0.4749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [371/855], Train Acc: 67.28070175438596\n",
      "tensor(0.4198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [381/855], Train Acc: 67.26608187134502\n",
      "tensor(0.5493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [391/855], Train Acc: 67.9093567251462\n",
      "tensor(0.9594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [401/855], Train Acc: 68.27485380116958\n",
      "tensor(0.6767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [411/855], Train Acc: 68.87426900584795\n",
      "tensor(0.6247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [421/855], Train Acc: 67.96783625730994\n",
      "tensor(0.7795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [431/855], Train Acc: 68.94736842105263\n",
      "tensor(0.5908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [441/855], Train Acc: 67.63157894736842\n",
      "tensor(0.5379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [451/855], Train Acc: 67.96783625730994\n",
      "tensor(0.7192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [461/855], Train Acc: 68.85964912280701\n",
      "tensor(0.6380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [471/855], Train Acc: 69.82456140350877\n",
      "tensor(0.6151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [481/855], Train Acc: 70.0\n",
      "tensor(0.5196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [491/855], Train Acc: 69.4298245614035\n",
      "tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [501/855], Train Acc: 67.23684210526316\n",
      "tensor(0.5620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [511/855], Train Acc: 67.99707602339181\n",
      "tensor(0.4359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [521/855], Train Acc: 68.17251461988305\n",
      "tensor(0.4802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [531/855], Train Acc: 70.0\n",
      "tensor(0.4348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [541/855], Train Acc: 69.66374269005848\n",
      "tensor(0.4229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [551/855], Train Acc: 70.05847953216374\n",
      "tensor(0.6123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [561/855], Train Acc: 68.61111111111111\n",
      "tensor(0.6577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [571/855], Train Acc: 70.23391812865498\n",
      "tensor(0.7087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [581/855], Train Acc: 69.51754385964912\n",
      "tensor(0.5268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [591/855], Train Acc: 69.34210526315789\n",
      "tensor(0.3691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [601/855], Train Acc: 69.78070175438596\n",
      "tensor(0.6258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [611/855], Train Acc: 69.89766081871345\n",
      "tensor(0.4609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [621/855], Train Acc: 69.85380116959064\n",
      "tensor(0.4340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [631/855], Train Acc: 69.47368421052632\n",
      "tensor(0.7621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [641/855], Train Acc: 69.66374269005848\n",
      "tensor(0.7527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [651/855], Train Acc: 69.03508771929825\n",
      "tensor(0.5411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [661/855], Train Acc: 69.86842105263158\n",
      "tensor(0.5928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [671/855], Train Acc: 69.47368421052632\n",
      "tensor(0.6447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [681/855], Train Acc: 69.40058479532163\n",
      "tensor(1.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [691/855], Train Acc: 70.05847953216374\n",
      "tensor(0.6714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [701/855], Train Acc: 70.13157894736842\n",
      "tensor(0.7311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [711/855], Train Acc: 70.24853801169591\n",
      "tensor(0.7644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [721/855], Train Acc: 67.42690058479532\n",
      "tensor(0.6809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [731/855], Train Acc: 70.48245614035088\n",
      "tensor(0.4431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [741/855], Train Acc: 70.39473684210526\n",
      "tensor(0.4061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [751/855], Train Acc: 70.30701754385964\n",
      "tensor(0.5353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [761/855], Train Acc: 70.2046783625731\n",
      "tensor(0.4120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [771/855], Train Acc: 70.17543859649123\n",
      "tensor(0.4189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [781/855], Train Acc: 70.29239766081871\n",
      "tensor(0.3903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [791/855], Train Acc: 69.09356725146199\n",
      "tensor(0.5582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [801/855], Train Acc: 69.95614035087719\n",
      "tensor(0.5575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [811/855], Train Acc: 70.26315789473684\n",
      "tensor(0.5377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [821/855], Train Acc: 70.43859649122807\n",
      "tensor(0.5962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [831/855], Train Acc: 69.69298245614036\n",
      "tensor(0.5674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [841/855], Train Acc: 70.42397660818713\n",
      "tensor(0.4692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [4/5], Step: [851/855], Train Acc: 70.05847953216374\n",
      "tensor(0.7448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [11/855], Train Acc: 69.50292397660819\n",
      "tensor(0.7730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [21/855], Train Acc: 67.13450292397661\n",
      "tensor(0.7073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [31/855], Train Acc: 68.80116959064327\n",
      "tensor(0.5793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [41/855], Train Acc: 69.00584795321637\n",
      "tensor(0.5796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [51/855], Train Acc: 69.92690058479532\n",
      "tensor(0.4828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [61/855], Train Acc: 70.01461988304094\n",
      "tensor(0.4098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [71/855], Train Acc: 70.30701754385964\n",
      "tensor(0.5503, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/5], Step: [81/855], Train Acc: 70.5701754385965\n",
      "tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [91/855], Train Acc: 68.24561403508773\n",
      "tensor(0.3554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [101/855], Train Acc: 71.14035087719299\n",
      "tensor(0.5039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [111/855], Train Acc: 71.09649122807018\n",
      "tensor(0.5665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [121/855], Train Acc: 67.83625730994152\n",
      "tensor(0.5633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [131/855], Train Acc: 68.45029239766082\n",
      "tensor(0.6775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [141/855], Train Acc: 71.00877192982456\n",
      "tensor(0.7636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [151/855], Train Acc: 69.16666666666667\n",
      "tensor(0.5335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [161/855], Train Acc: 70.43859649122807\n",
      "tensor(0.7300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [171/855], Train Acc: 68.9327485380117\n",
      "tensor(0.6557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [181/855], Train Acc: 68.50877192982456\n",
      "tensor(0.7273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [191/855], Train Acc: 70.65789473684211\n",
      "tensor(0.4978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [201/855], Train Acc: 70.55555555555556\n",
      "tensor(0.6510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [211/855], Train Acc: 70.04385964912281\n",
      "tensor(0.3499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [221/855], Train Acc: 67.48538011695906\n",
      "tensor(0.5201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [231/855], Train Acc: 70.42397660818713\n",
      "tensor(0.6688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [541/855], Train Acc: 70.4093567251462\n",
      "tensor(0.4075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [551/855], Train Acc: 71.2719298245614\n",
      "tensor(0.5802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [561/855], Train Acc: 68.87426900584795\n",
      "tensor(0.5706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [571/855], Train Acc: 71.09649122807018\n",
      "tensor(0.6225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [581/855], Train Acc: 71.21345029239767\n",
      "tensor(0.4993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [591/855], Train Acc: 71.14035087719299\n",
      "tensor(0.3812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [601/855], Train Acc: 71.46198830409357\n",
      "tensor(0.6826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [611/855], Train Acc: 71.57894736842105\n",
      "tensor(0.5598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [621/855], Train Acc: 71.19883040935673\n",
      "tensor(0.4229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [631/855], Train Acc: 70.0\n",
      "tensor(0.7383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [641/855], Train Acc: 70.02923976608187\n",
      "tensor(0.7362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [651/855], Train Acc: 69.37134502923976\n",
      "tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [661/855], Train Acc: 71.3157894736842\n",
      "tensor(0.5415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [671/855], Train Acc: 71.09649122807018\n",
      "tensor(0.6298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [681/855], Train Acc: 70.92105263157895\n",
      "tensor(0.9577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [691/855], Train Acc: 71.65204678362574\n",
      "tensor(0.7186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [701/855], Train Acc: 71.62280701754386\n",
      "tensor(0.7088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [711/855], Train Acc: 71.69590643274854\n",
      "tensor(0.7621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [721/855], Train Acc: 68.6842105263158\n",
      "tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [731/855], Train Acc: 71.73976608187135\n",
      "tensor(0.3956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [741/855], Train Acc: 71.76900584795321\n",
      "tensor(0.3481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [751/855], Train Acc: 71.44736842105263\n",
      "tensor(0.4715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [761/855], Train Acc: 71.4327485380117\n",
      "tensor(0.3640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [771/855], Train Acc: 71.34502923976608\n",
      "tensor(0.4309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [781/855], Train Acc: 71.79824561403508\n",
      "tensor(0.3335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [791/855], Train Acc: 70.58479532163743\n",
      "tensor(0.5226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [801/855], Train Acc: 71.56432748538012\n",
      "tensor(0.5380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [811/855], Train Acc: 71.46198830409357\n",
      "tensor(0.4635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [821/855], Train Acc: 71.53508771929825\n",
      "tensor(0.5915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [831/855], Train Acc: 70.1608187134503\n",
      "tensor(0.5652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [841/855], Train Acc: 71.0233918128655\n",
      "tensor(0.4086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch: [5/5], Step: [851/855], Train Acc: 70.13157894736842\n"
     ]
    }
   ],
   "source": [
    "embedding = torch.FloatTensor(_WEIGHTS)\n",
    "model = GRU(embedding, embedding_size = 300, hidden_size=350, num_layers=1, num_classes=2).to(device)\n",
    "max_val_acc, losses, xs, val_accs = train_model(model, num_epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
